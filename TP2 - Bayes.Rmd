---
title: |
  \vspace{1cm}
  \begin{tabular}{c}
  {\normalsize\textbf{UNIVERSIDAD NACIONAL DE ROSARIO}}\\
  {\Large Facultad de Ciencias Económicas y Estadística}\\
  \\
  \includegraphics[width=5cm]{LogoUNR.png}\\
  \vspace{1cm}
  \\
  {\huge\textbf{"Metropolis-Hastings"}}\\
  {\Large Estadística Bayesiana - Trabajo Práctico Nº2}\\
  \end{tabular}
  \vspace{5cm}
author: |
  *Alumnas:* Agustina Mac Kay, Ailén Salas y Rocío Canteros
date: "Año 2024"
output: pdf_document
editor_options: 
  markdown: 
    wrap: sentence
---

# Introducción

```{r setup, warning=FALSE, message=FALSE, echo=FALSE}
#Librerias
library(ggplot2)
library(dplyr)
library(gridExtra)
library(kableExtra)
library(stats)
library(mvtnorm)
set.seed(394)
```

El algoritmo de Metropolis-Hastings (MH) permite generar muestras (pseudo-)aleatorias a partir de una distribución de probabilidad $P$ que no necesariamente pertence a una familia de distribuciones conocida.
El único requisito es que se pueda evaluar la función de densidad (o de masa de probabilidad) $p^*(\theta)$ en cualquier valor de $\theta$, incluso cuando $p^*(\theta)$ sea impropia (es decir, incluso aunque sea desconocida la constante de normalización que hace que la integral en el soporte de la función sea igual a uno).

Los pasos del algoritmo son:

1\.
Durante la iteración $i$, se encuentra en el valor del parámetro $\theta^{(i)}$.

2\.
En función del valor de parámetro actual $\theta^{(i)} = \theta$, se propone un nuevo valor $\theta '$ en función de $q(\theta'|\theta)$.

3\.
Se decide si se vá a la nueva ubicación $\theta^{(i+1)} = \theta'$ o si se queda $\theta^{(i+1)} = \theta$:

-   Se calcula la probabilidad de salto:

$$\alpha_{\theta \rightarrow \theta'} = \min \left\{1, \frac{f(\theta')}{f(\theta)} \frac{q(\theta|\theta')}{q(\theta'|\theta)}\right\}$$

-   Pasar a $\theta'$ con probabilidad $\alpha_{\theta \rightarrow \theta'}$:

$$ \theta^{(i+1)} = \left\{
\begin{matrix}
\theta & con & probabilidad & \alpha_{\theta \rightarrow \theta'}   \\
\theta & con & probabilidad & (1-\alpha_{\theta \rightarrow \theta'})
\end{matrix}
\right.$$

A continuación, se presenta la función que implementa el algoritmo de Metropolis-Hastings para tomar muestras de una distribución de probabilidad a partir de su función de densidad.
Se otorga flexibilidad al algoritmo permitiendo elegir entre un punto de inicio arbitrario o al azar y permitiendo utilizar distribuciones de propuesta de transición arbitrarias (por defecto, se utiliza una distribución normal estándar).
\newpage

```{r, echo=TRUE, warning=FALSE, message=FALSE}
# Función de Metropolis-Hastings

cant_saltos <- 0  # se inicia en 0 el contador de saltos

sample_mh <- function(d_objetivo, r_propuesta, d_propuesta, p_inicial, n) {
  muestras <- matrix(nrow = n, ncol = length(p_inicial))
  muestras[1, ] <- p_inicial
  
  for(i in 2:n) {
    p_actual <- muestras[i-1,]
    p_nuevo <- r_propuesta(p_actual)
    
    f_nuevo <- d_objetivo(p_nuevo)
    f_actual <- d_objetivo(p_actual)
    
    q_actual <- d_propuesta(p_actual, mean = p_nuevo)
    q_nuevo <- d_propuesta(p_nuevo, mean = p_actual)
    
    alpha <- min(1, (f_nuevo/f_actual)*(q_actual/q_nuevo))
    aceptar <- rbinom(1, 1, alpha)
    
    if(aceptar) { 
      muestras[i,] <- p_nuevo
      cant_saltos <- cant_saltos + 1
      
    } else {
      muestras[i,] <- p_actual
    }
  }
  
  if (ncol(muestras) == 1) {
    muestras <- as.vector(muestras)
  } 
  return(list(muestras=muestras,cant_saltos=cant_saltos))
}

```

\newpage

# Metropolis-Hastings en 1D

## Distribución de Kumaraswamy

La distribución de Kumaraswamy es una distribución de probabilidad continua que se utiliza para modelar variables aleatorias con soporte en el intervalo $(0,1)$.
Si bien graficamente la forma de su función de densidad puede hacer recordar a la distribución beta, vale mencionar que la distribución de Kumaraswamy resulta en una expresión matemática cuyo cómputo es más sencillo:

$$p(x|a,b) = abx^{a-1}(1-x^a)^{b-1}$$ con $a,b > 0$

A continuación, se grafica la función de densidad de la distribución de Kumaraswamy para las combinaciones de los parámetros:

$$
\begin{array}{ccc}
Combinación & a & b \\
\hline
1 & 0.2 & 0.2 \\
2 & 0.1 & 0.7 \\
3 & 3 & 3 \\
4 & 4 & 9 \\
5 & 10 & 5 \\
\end{array}
$$

```{r, echo=FALSE, message=FALSE, warning=FALSE}
#Punto 2
# Crear grilla para los valores de "x"
grid_n <- 200
x_grid <- seq(0, 1, length.out = grid_n)

kumaraswamy <- function(x, a, b){
  a*b*(x^(a-1))*((1-(x^a))^(b-1))
}

a <- c(0.2, 0.1, 3, 4, 10)
b <- c(0.2, 0.7, 3, 9, 5)


#Creamos un data frame para graficar la distribución de Kumaraswamy:

data1 <- data.frame(
  Funcion = as.factor(rep(1:5, each = grid_n)),
  Densidad = numeric(5 * grid_n),
  Grilla = rep(x_grid, times = 5)
) 

# Completamos el data frame con las densidades:
for(i in 1:5) {
    indices <- seq(from = 1 + (i - 1) * 200, to = 200 + (i - 1) * 200)
    data1$Densidad[indices] <- kumaraswamy(x_grid, a[i], b[i])
}

# Cambiamos las etiquetas de las variables para mejorar el gráfico
levels(data1$Funcion) <- c("a = 0.2, b = 0.2",
                           "a = 0.1, b = 0.7",
                           "a = 3, b = 3",
                           "a = 4, b = 9",
                           "a = 10, b = 5")

ggplot(data = data1, aes(x = Grilla, y = Densidad)) +
  geom_line(size = 0.55) +
  facet_wrap(~Funcion) +
  theme_bw() +
  labs(x = "x",
       caption = "Gráfico 1: Distribución Kumaraswamy con distintas combinaciones de los parámetros a y b") + 
  theme(
    strip.background = element_rect(fill = "olivedrab3"),
    plot.caption = element_text(hjust = 0.5)
  )

```

En el gráfico 1 se puede apreciar las distintas formas que toman las curvas de la distribución Kumaraswamy dependiendo de los parámetros a y b que se elijan.
El parámetro $a$ controla la asimetría de la curva.
El parámetro $b$ controla la curvatura de la gráfica.
Se espera que si $a = b$, la curva sea simétrica.
Si $a > b$, la curva se inclina hacia la derecha.
Si $a < b$, la curva se inclina hacia la izquierda.
Se observa que:

-   Si los parámetros son iguales y menores a 1, la curva es simétrica y tiene forma de U.

-   Si los dos parámetros son menores a 1 y $a < b$, la curva tiene forma de U y es más aplastada del lado derecho.

-   Si los parámetros son iguales y mayores a 1, la curva es simétrica y tiene forma de campana.

-   Si ambos parámetros son mayores a 1 y $a < b$, la curva tiene forma de campana.

-   Si ambos parámetros son mayores a 1 y $a > b$, la curva es asimétrica a la izquierda y tiene forma de campana.

Conocer las distintas formas que puede tomar la curva de la distribución de Kumaraswamy según los parámetros $a$ y $b$ es útil en Estadística Bayesiana porque:

-   Facilita la elección de un prior que refleje adecuadamente las creencias previas sobre los parámetros del modelo.
    Esto es crucial para obtener inferencias precisas y robustas.

-   Permite adapar el modelo a diferentes tipos de datos, ya que la distribución puede variar ampliamente de forma dependiendo de los valores de $a$ y $b$.

-   Ayuda a comprender cómo los valores de los parámetros afectan el posterior y, por lo tanto, las conclusiones que se pueden extraer del análisis.

Utilizando la función construída al comienzo, se obtienen 5000 muestras de una distribución Kumaraswamy con parámetros $a=6$ y $b=2$.
Como distribución propuesta se utiliza una beta con los siguientes grados de concentración:

$$
\begin{array}{cccc}
Concentración & 4 & 10 & 20 \\
\end{array}
$$

Como punto inicial del algoritmo de MH, se obtiene un valor aleatorio de una distribución $beta(2,2)$

La tasa de aceptación en el algoritmo de Metropolis-Hastings indica qué tan frecuentemente se aceptan los nuevos $\theta$ propuestos, en relación al total de $\theta$ propuestos.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
#Punto 3
n3 <- 2000
concentracion3 <- c(4,10,20)
tabla3 <- data.frame ( 
  concentracion = rep(concentracion3, each = 2000),
  muestra = numeric(6000)
  )
tasa3 <- numeric(3)

for (i in 1:3) {
#Funciones a usar
d_objetivo3 <- function(x) kumaraswamy(x, 6, 2)
d_propuesta3 <- function(x, mean) dbeta(x, shape1 = mean * concentracion3[i], shape2 = (1-mean) * concentracion3[i])
r_propuesta3 <- function(x) rbeta(1, shape1 = x * concentracion3[i], shape2 = (1-x) * concentracion3[i])
#Donde x hace referencia a mu

p_inicial3 <- rbeta(1,shape1=2,shape2=2)
funcion3 <- sample_mh(d_objetivo3, r_propuesta3, d_propuesta3, p_inicial3, n3)
indices <- seq(from = 1 + (i - 1) * 2000, to = 2000 + (i - 1) * 2000)
tabla3$muestra[indices] <- funcion3$muestras

tasa3[i] <- funcion3$cant_saltos/n3
} 

#Tasa de aceptacion
  data.frame(
  Concentracion = c("4","10","20"),
  Tasa = round(tasa3, digits = 2)
) %>%
  kable(align = "c") %>%
  kable_styling(bootstrap_options = c("striped","hover","condensed"),
                latex_options = c("hold_position")) %>%
    add_header_above(c(" " = 1, "Tabla 1:Tasa de aceptación para cada concentración" = 1))
```

En la tabla 1 se observa que, a mayor concentración de la distribución propuesta beta, mayor es la tasa de aceptación.

A continuación, una representación gráfica que muestra cómo evolucionan las muestras generadas por el algoritmo a lo largo del tiempo.

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.height=5}

#Trace plots
plot_trace <- function(x){
  data.frame(x = seq_along(x), y = x) %>% 
    ggplot() +
    geom_line(aes(x = x, y = y), color = "slateblue")+
    theme_bw() +
    theme(plot.title = element_text(size = 12))
}

grafico3_2 <- plot_trace(tabla3$muestra[1:2000]) +
              labs(x = "", y = expression(theta), 
                   title = expression(kappa ~ " = 4"))
grafico3_3 <- plot_trace(tabla3$muestra[2001:4000]) +
              labs(x = "", y = expression(theta), 
                   title = expression(kappa ~ " = 10"))
grafico3_4 <- plot_trace(tabla3$muestra[4001:6000]) +
              labs(x = "iteración", y = expression(theta), 
                   title = expression(kappa ~ " = 20"))

grid.arrange(grafico3_2,grafico3_3,grafico3_4, nrow=3, bottom ="Gráfico 2: Plot trace según concentración")
```

El gráfico 2 muestra que para las 3 concentraciones elegidas, el trace plot resulta ser un ruido blanco sin ningún patrón particular.
Sin embargo, aunque es dificil apreciar de manera detallada el comportamiento del algoritmo debido a la cantidad de muestras, pareciera ser que el de concentración $k=10$ tiene un comportamiento más apropiado.
Esto se debe a que se puede apreciar algunos estancamientos del valor de $\theta$ en los de concentraciones $k=4$ y $k=20$.

Para evaluar la convergencia de las muestras a la distribución objetivo se presenta:

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.height=2.5}

#Histogramas
plot_hist <- function(x, d_objetivo3) {
  x_seq <- seq(min(x), max(x), length.out = n3)
  df_hist <- data.frame(x = x)
  df_line <- data.frame(x = x_seq, y = d_objetivo3(x_seq))
  
  ggplot(df_hist)+
    geom_histogram(aes(x = x, y =after_stat(density)), fill = "slategray3") +
    geom_line(aes(x = x, y = y), data = df_line, 
              color = "slateblue", size = 0.8) +
    ylim(c(0,4)) +
    theme_bw()
}

grafico3_5 <- plot_hist(tabla3$muestra[1:2000],d_objetivo3) +
  labs (x = "Muestras", y = "Densidad",
        title = expression(kappa ~ " = 4"))
grafico3_6 <- plot_hist(tabla3$muestra[2001:4000],d_objetivo3) +
  labs (x = "Muestras", y = "Densidad",
        title = expression(kappa ~ " = 10"))
grafico3_7 <- plot_hist(tabla3$muestra[4001:6000],d_objetivo3) +
  labs (x = "Muestras", y = "Densidad",
        title = expression(kappa ~ " = 20"))

grid.arrange(grafico3_5,grafico3_6,grafico3_7,nrow=1, bottom = "Gráfico 3: Muestras obtenidas y distribución de kumaraswamy según concentración")

```

\newpage

En el gráfico 3 se observa que las muestras generadas para los 3 valores de concentración se ajustan a la distribución objetivo.
Las 3 muestras exploran el rango completo de la distribución a posteriori.
Sin embargo, pareciera que las concentraciones $k=10$ y $k=20$ ajustan mejor (a excepción de una barra muy alta en $k=10$)

Se calcula la correlación de la serie para cada valor de lag k consigo misma originando la función de autocorrelación:

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.height=3}

#Graficos de autocorrelacion
plot_acf <- function(data, title) {
  acf_result <- acf(data, plot = FALSE)
  ggplot() +
    geom_line(aes(y = acf_result$acf, x = acf_result$lag), color = "olivedrab3", size = 0.8) +
    labs(title = title, x = "Rezago", y = "Autocorrelación") +
    theme_bw()
}


grafico3_8 <- plot_acf(tabla3$muestra[1:2000],
                       expression(kappa ~ " = 4"))
grafico3_9 <- plot_acf(tabla3$muestra[2001:4000], 
                       expression(kappa ~ " = 10"))
grafico3_10 <- plot_acf(tabla3$muestra[4001:6000], 
                        expression(kappa ~ " = 20"))

grid.arrange(grafico3_8,grafico3_9,grafico3_10,nrow=1,  bottom = "Gráfico 4: Autocorrelación según concentración")

```

Las muestras tienen que ser independientes.
La dependencia de valores anteriores tiene que desaparecer rápido.
En el gráfico 4 se observa que esto ocurre para los 3 valores de concentración.
Sin embargo, a diferencia de los resultados obtenidos anteriormente, este gráfico sugiere que el valor más adecuado para la concentración es $k=4$, teniendo una correlación de 0.25 en el rezago 5.
Cabe destacar que la diferecia con las otras concentraciones no es grande, siendo éstas de 0.25 en los rezagos 6 y 8 aproximadamente.

Para cada una de las cadenas anteriores, se presentan la media de la distribución y los percentiles de $X$:

```{r, echo=FALSE, warning=FALSE, message=FALSE}
#Punto 4
muestra4_1 <- tabla3$muestra[1:2000]
muestra4_2 <- tabla3$muestra[2001:4000]
muestra4_3 <- tabla3$muestra[4001:6000]

#Calculo de la media y los percentiles 5 y 95 para X:
data.frame(
  Muestra = c("1", "2", "3"),
  Concentración = c("k=4", "k=10", "k=20"),
  Medias = round(c(mean(muestra4_1), mean(muestra4_2), mean(muestra4_3)), digits = 2),
  Percentil_5 = round(c(quantile(muestra4_1, prob = c(0.05)),
                  quantile(muestra4_2, prob = c(0.05)),
                  quantile(muestra4_3, prob = c(0.05))
                  ), digits = 2),
  Percentil_95 = round(c(quantile(muestra4_1, prob = c(0.95)),
                  quantile(muestra4_2, prob = c(0.95)),
                  quantile(muestra4_3, prob = c(0.95))
                  ), digits = 2)
) %>%
  kable(align = "c", col.names = c("Muestra", "Concentración", "Media", "Percentil del 5%", "Percentil del 95%")) %>%
  kable_styling(bootstrap_options = c("striped","hover","condensed"),
                latex_options = c("hold_position")) %>%
  add_header_above(c(" " = 2, "Tabla 2: Media y percentiles de X" = 3))
```

Para los 3 valores de concentración, se obtienen resultados muy similares para la media (0.79) y para los percentiles 5 y 95 de la distribución, siendo éstos 0.55 y 0.96 respectivamente.

Para cada una de las concentraciones, se presentan la media y los percentiles de la distribución de $logit(X)$:

```{r, echo=FALSE, warning=FALSE, message=FALSE}
#Cálculo de la media y los percentiles 5 y 95 para logit(X):
l_m1 <- log((muestra4_1/(1-muestra4_1)))
l_m2 <- log((muestra4_2/(1-muestra4_2)))
l_m3 <- log((muestra4_3/(1-muestra4_3)))



data.frame(
  Muestra = c("1", "2", "3"),
  Concentración = c("k=4", "k=10", "k=20"),
  Medias = round(c(mean(l_m1), mean(l_m2), mean(l_m3)), digits = 2),
  Percentil_5 = round(c(quantile(l_m1, prob = c(0.05)),
                  quantile(l_m2, prob = c(0.05)),
                  quantile(l_m3, prob = c(0.05))), digits = 2),
  Percentil_95 = round(c(quantile(l_m1, prob = c(0.95)),
                   quantile(l_m2, prob = c(0.95)),
                   quantile(l_m3, prob = c(0.95))), digits = 2)
  ) %>%
  kable(align = "c", col.names = c("Muestra", "Concentración", "Media", "Percentil del 5%", "Percentil del 95%")) %>%
  kable_styling(bootstrap_options = c("striped","hover","condensed"),
                latex_options = c("hold_position")) %>%
  add_header_above(c(" " = 2, "Tabla 3: Media y percentiles de logit(X)" = 3))
```

Al hacer uso de todo el eje real mediante el $logit(x)$, se observa que la media para $k=20$ difiere un poco de las medias correspondientes a las otras 2 concentraciones.
A su vez, los percentiles 5 y 95 que difieren más son los de concentración $k=4$.
Cabe destacar que no se consideran relevantes estas diferencias.

### Conclusión

Al analizar las distintas muestras de distribución Kumaraswamy de parámetros $a=6$ y $b=2$ con propuesta beta de distintas concentraciones, no se encontraron grandes diferencias entre éstas.
No es posible decidir con certeza cuál de los valores de concentración es mejor, debido a que las diferencias obtenidas han sido muy pequeñas.
Sin embargo, si habría que sugerir un valor de concentración, se sugiere el 10.
Esto es debido a que tiene una tasa de aceptación moderada (0.65), el algoritmo no se estanca demasiado en los mismos valores de $\theta$, la muestra explora el rango completo de la distribución a posteriori y se ajusta bien a la curva.
Además, la autocorrelación decrece rápidamente y es menor a 0.25 a partir del rezago 6.

\newpage

# Metropolis-Hastings en 2D

La verdadera utilidad del algoritmo de Metropolis-Hastings se aprecia cuando se obtienen muestras de distribuciones en más de una dimensión, incluso cuando no se conoce la constante de normalización.
En esta sección se trabaja con ejemplos que permitirán advertir las limitaciones del algoritmo y motivarán la búsqueda de mejores alternativas.

## Normal multivariada

La distribución normal multivariada es la generalización de la distribución normal univariada a múltiples dimensiones (o mejor dicho, el caso en una dimensión es un caso particular de la distribución en múltiples dimensiones).
La función de densidad de la distribución normal en $k$ dimensiones es:

$$p(x|\mu,\Sigma) = \frac{1}{(2\pi)^{k/2}\lvert \Sigma \rvert^{1/2}} exp(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))$$ donde $\mu$ es el vector de medias y $\Sigma$ la matriz de covarianza.

Utilizando la función descrita al principio del informe se obtienen muestras de una distribución normal bivariada con media $\mu^*$ y matriz de covarianza $\Sigma^*$.

$$\mathbf{\mu^*} = \begin{bmatrix}
0.4 \\
0.75
\end{bmatrix}$$

$$\mathbf{\Sigma^*} = \begin{bmatrix}
1.35 & 0.4 \\
0.4 & 2.4
\end{bmatrix}$$

Con el objetivo de analizar cual es la matriz de covarianza para la distribución propuesta más óptima, se prueba con las siguientes:

$$\mathbf{\Sigma^1} = \begin{bmatrix}
2 & 0.5\\
0.5 & 31.35 
\end{bmatrix}$$

$$\mathbf{\Sigma^2} = \begin{bmatrix}
2 & 0\\
0 & 3 
\end{bmatrix}$$

$$\mathbf{\Sigma^3} = \begin{bmatrix}
0.4 & 0\\
0 & 0.4 
\end{bmatrix}$$

$$\mathbf{\Sigma^4} = \begin{bmatrix}
1 & 0\\
0 & 1 
\end{bmatrix}$$

$$\mathbf{\Sigma^5} = \begin{bmatrix}
5 & 0\\
0 & 6 
\end{bmatrix}$$

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.height=2.8}
#Paso 7

# Funciones a utilizar

#1.Trace plot
plot_trace <- function(x, y) {
  n <- length(x)
  df <- data.frame(
    x = rep(1:n, 2),
    y = c(x,y),
    parametro = factor(rep(1:2, each = n), labels = c(expression(mu[1]), expression(mu[2]))) 
  )
  
  ggplot(df) +
    geom_line(aes(x = x, y = y, color = parametro)) +
    facet_wrap(~parametro, labeller = label_parsed,
               nrow = 2, strip.position = "right") +
    scale_color_manual(values = c("olivedrab3","slateblue")) +
    labs(x = "iteración", y = "Valor") +
    theme_bw()
}



#2. Número efectivo de muestras
n_eff <- function(muestras){
  S <- length(muestras)
  autocorr <- acf(muestras, plot = F, lag = Inf)$acf
  limite <- which((autocorr < 0.001))[1] #sumamos hasta ese valor
  denom <- 1 + 2 * sum(autocorr[2:limite])
  
  S/denom
  
}


n7 <- 5000
funcion7_1 <- function(matriz_cov) {
    d_objetivo7 <- function(x) dmvnorm(x,c(0.4,0.75),matrix(c(1.35,0.4,0.4,2.4),nrow=2))
    d_propuesta7 <- function(x, mean) dmvnorm(x,mean = mean ,sigma = matriz_cov)
    r_propuesta7 <- function(x) rmvnorm(1, mean = x, sigma = matriz_cov)  


funcion7 <- sample_mh(d_objetivo7,r_propuesta7,d_propuesta7,c(0,0),n7)
muestra7 <- funcion7$muestras
grafico7_1 <- plot_trace(muestra7[,1],muestra7[,2])
n_eff7_1 <- n_eff(muestra7[,1])
n_eff7_2 <- n_eff(muestra7[,2])
 return(list(grafico = grafico7_1, n1 = n_eff7_1, n2 = n_eff7_2,
             muestra7 = muestra7))

}

#Evalucación de diferentes matrices de variancia y covariancia
matriz1 <- funcion7_1(matrix(c(2, 0.5, 0.5, 3), nrow = 2))
matriz2 <- funcion7_1(matrix(c(2, 0, 0, 3), nrow = 2))
matriz3 <- funcion7_1(matrix(c(0.4, 0, 0, 0.4), nrow = 2))
matriz4 <- funcion7_1(matrix(c(1, 0, 0, 1), nrow = 2))
matriz5 <- funcion7_1(matrix(c(5, 0, 0, 6), nrow = 2))

matriz1$grafico + labs(caption = "Gráfico 5: Trace plot de la matriz 1") +
    theme(legend.position = "none")
matriz2$grafico + labs(caption = "Gráfico 6: Trace plot de la matriz 2") +
    theme(legend.position = "none")
matriz3$grafico + labs(caption = "Gráfico 7: Trace plot de la matriz 3") +
    theme(legend.position = "none")
matriz4$grafico + labs(caption = "Gráfico 8: Trace plot de la matriz 4") +
    theme(legend.position = "none")
matriz5$grafico + labs(caption = "Gráfico 9: Trace plot de la matriz 5") +
    theme(legend.position = "none")

```

En los gráficos 5 a 9 se observa que, para los dos parámetros, las iteraciones oscilan alrededor del cero.
Puesto que la cantidad de observaciones es elevada, no se aprecian con claridad los estancamientos del valor de $\theta$.
Pareciera que con la matriz 3 es con la cual más se estanca.

```{r, echo=FALSE, warning=FALSE, message=FALSE}

data.frame(
  Matriz = c("1", "2", "3", "4", "5"),
  Num_efectivo_p1 = round(c(matriz1$n1, matriz2$n1, matriz3$n1, matriz4$n1, matriz5$n1), digits = 2),
  Num_efectivo_p2 = round(c(matriz1$n2, matriz2$n2, matriz3$n2, matriz4$n2, matriz5$n2), digits = 2)
  ) %>%
  kable(align = "c",col.names = c("Matriz", "mu_1", "mu_2")) %>%
  kable_styling(bootstrap_options = c("striped","hover","condensed"),
                latex_options = c("hold_position")) %>%
  add_header_above(c(" " = 1,"Tabla 4: Números efectivos de muestras para\ncada matriz y cada parámetro" = 2))

#Nos quedamos con la ultima matriz.
```

En la tabla 4 se observa que la matriz de covarianza de la distribución propuesta que devuelve un valor más óptimo de muestras efectivas es la matriz 5.
Teniendo en cuenta que la cantidad de muestras es 5000, este resulta ser un valor bajo.
De todas maneras, al ser el más alto, se elige esta matriz.

A partir de las muestras obtenidas con la matriz 5, se estiman las siguientes probabilidades:

i\.
$Pr(X_1>1,X_2>0)$

ii\.
$Pr(X_1>1,X_2>2)$

iii\.
$Pr(X_1>0.4,X_2>0.75)$

Luego, mediante la función de la distribución de la normal bivariada se obtienen las probabilidades reales con el objetivo de compararlas y ver si se obtuvo una buena muestra con el método anterior.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
#Punto 8: se calculan las probabilidades con la última matriz de var y cov.
muestra7 <- as.data.frame(matriz5$muestra7)
colnames(muestra7) <- c("X", "Y")

#Probabilidades estimadas
#Probabilidad n°1:
prob1 <- muestra7 %>% 
  filter(X > 1) %>% 
  count(Y < 0)

p1 <- prob1$n[2]/(n7)


#Probabilidad n°2:
prob2 <- muestra7 %>% 
  filter(X > 1) %>% 
  count(Y > 2)

p2 <- prob2$n[2]/(n7)



#Probabilidad n°3:
prob3 <- muestra7 %>% 
  filter(X > 0.4) %>% 
  count(Y > 0.75)

p3 <- prob3$n[2]/(n7)



#Probabilidades reales:

#Probabilidad n°1:
p4 <- pmvnorm(lower = c(1, -Inf), upper = c(Inf, 0), 
        mean = c(0.4, 0.75), sigma = matrix(c(1.35, 0.4, 0.4, 2.4), nrow = 2))

#Cómo funcionan "lower" y "upper"? 
#"lower" es un vector que indica los limites inferiores de cada variable: 1 para X y -Infinito para Y; "upper" funciona de manera igual.


#Probabilidad n°2:
p5 <- pmvnorm(lower = c(1, 2), upper = c(Inf, Inf), 
        mean = c(0.4, 0.75), sigma = matrix(c(1.35, 0.4, 0.4, 2.4), nrow = 2))



#Probabilidad n°3:
p6 <- pmvnorm(lower = c(0.4, 0.75), upper = c(Inf, Inf), 
        mean = c(0.4, 0.75), sigma = matrix(c(1.35, 0.4, 0.4, 2.4), nrow = 2))

data.frame (
  Probabilidad = c("i","ii","iii"),
  Estimada = round(c(p1,p2,p3), digits = 2),
  Real = round(c(p4,p5,p6), digits = 2)
) %>%
  kable(align = "c") %>%
  kable_styling(bootstrap_options = c("striped","hover","condensed"),
                latex_options = c("hold_position")) %>%
  add_header_above(c(" " = 1, "Tabla 5: Probabilidades estimadas y reales" = 2))
```

Según lo observado en la tabla 5, se podría considerar que se tiene una buena muestra de una normal bivariada con media $\mu^*$ y matriz de covarianza $\Sigma^*$ a través del algoritmo de Metrópolis-Hastings en 2D, con la matriz de covarianza para la distribución propuesta $\Sigma^5$.

### Conclusión

Utilizando el algoritmo de Metrópolis-Hastings en 2D para generar muestras de una distribución normal bivariada con la media y la matriz de varianza y covarianza especificadas, se observa que la elección de la matriz de varianza y covarianza de la distribución propuesta ($\mathbf{\Sigma^5}$) influye en la eficiencia del muestreo.
Al comparar con las otras matrices de varianza y covarianza, $\mathbf{\Sigma^5}$ presenta un número efectivo de muestras más alto, lo que indica una exploración más eficiente del espacio de parámetros.

## Función de Rosenbrock

La función de Rosenbrock, a veces llamada el “valle de Rosenbrock”, y comunmente conocida como la “banana de Rosenbrock”, es una función matemática utilizada frecuentemente como un problema de optimización y prueba para algoritmos de optimización numérica.

La función está definida por:

$$f(x,y)=(a-x)^2+b(y-x^2)^2$$

y cuenta con un mínimo global en $(x,y)=(a,a^2)$, que satisface $f(a,a^2)=0$.

Debido a su forma peculiar, la función de Rosenbrock presenta desafíos particulares para los algoritmos de optimización, ya que tiene un valle largo y estrecho en el que la convergencia puede ser lenta.

```{=tex}
\begin{center}
    \includegraphics[width=7cm]{im1.png}
\end{center}
```
Esta forma de banana popularizada por Rosenbrock es también muy conocida en el campo de la estadística bayesiana, ya que en ciertos escenarios, la densidad del posterior toma una forma que definitivamente se asemeja a la banana de Rosenbrock.
Un ejemplo de este fenómeno es la función $p^*$:

$$p^*(x_1,x_2|a,b)=exp{{(-[(a-x_1)^2+b(x_2-x_1^2)^2]})}$$

```{=tex}
\begin{center}
    \includegraphics[width=7cm]{im2.png}
\end{center}
```
```{r, echo = FALSE, message = FALSE, warning = FALSE}
#Punto 9

p_estrella <- function(x, a, b){
  exp(-((((a - x[1])^2) + b * ((x[2] - x[1])^2))^2))
}



n9 <- 5000

funcion9_1 <- function(matriz_cov) {
    d_objetivo9 <- function(x) p_estrella(x, 0.5, 5)
    d_propuesta9 <- function(x, mean) dmvnorm(x, mean = mean, sigma = matriz_cov)
    r_propuesta9 <- function(x) rmvnorm(1, mean = x, sigma = matriz_cov)  
    
    funcion9 <- sample_mh(d_objetivo9,r_propuesta9,d_propuesta9,c(0,0),n9)
    muestra9 <- funcion9$muestras
    grafico9_1 <- plot_trace(muestra9[,1],muestra9[,2])
    
    n_eff9_1 <- n_eff(muestra9[,1])
    n_eff9_2 <- n_eff(muestra9[,2])
    tasa9 <- funcion9$cant_saltos/n9
    autocorr_1 <- acf(muestra9[,1], plot = F)
    autocorr_2 <- acf(muestra9[,2], plot = F)
    
    return(list(grafico = grafico9_1, n1 = n_eff9_1, n2 = n_eff9_2,
             muestra9 = muestra9, tasa9 = tasa9, autocorr_p1 = autocorr_1,
             autocorr_p2 = autocorr_2))

}


matriz1 <- funcion9_1(matrix(c(0.3, 0.1, 0.1, 0.2), nrow = 2))
matriz2 <- funcion9_1(matrix(c(2, 0, 0, 2), nrow = 2))
matriz3 <- funcion9_1(matrix(c(4, 1, 1, 4), nrow = 2))



grafico9_1_1 <- ggplot()+
  geom_line(aes(y = matriz1$autocorr_p1$acf, x = matriz1$autocorr_p1$lag),
            color = "olivedrab3", size = 0.8) +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5)) +
  labs (title = "Matiz 1 ", x = "Rezago", y = "Autocorrelación")

grafico9_1_2 <- ggplot()+
  geom_line(aes(y = matriz1$autocorr_p2$acf, x = matriz1$autocorr_p2$lag),
            color="slateblue", size = 0.8) +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5)) +
  labs (title = "Matriz 1 ", x = "Rezago", y = "Autocorrelación")


grafico9_2_1 <- ggplot()+
  geom_line(aes(y = matriz2$autocorr_p1$acf, x = matriz2$autocorr_p1$lag),
            color = "olivedrab3", size = 0.8) +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5)) +
  labs (title = "Matriz 2 ", x = "Rezago", y = "Autocorrelación")

grafico9_2_2 <- ggplot()+
  geom_line(aes(y = matriz2$autocorr_p2$acf, x = matriz2$autocorr_p2$lag),
            color = "slateblue", size = 0.8) +
  theme_bw()  +
  theme(plot.title = element_text(hjust = 0.5)) +
  labs (title = "Matriz 2", x = "Rezago", y = "Autocorrelación")

grafico9_3_1 <- ggplot()+
  geom_line(aes(y = matriz3$autocorr_p1$acf, x = matriz3$autocorr_p1$lag),
            color = "olivedrab3", size = 0.8) +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5)) +
  labs (title = "Matriz 3", x = "Rezago", y = "Autocorrelación")


grafico9_3_2 <- ggplot()+
  geom_line(aes(y = matriz3$autocorr_p2$acf, x = matriz3$autocorr_p2$lag),
            color = "slateblue", size = 0.8) +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5)) +
  labs (title = "Matriz 3", x = "Rezago", y = "Autocorrelación")
```

```{r,echo = FALSE, fig.height=2.8}
matriz1$grafico + theme(legend.position = "none") + ggtitle("Matriz 1")
matriz3$grafico + labs(caption = "Gráfico 10: Plot trace de las matrices") + 
  theme(legend.position = "none") + ggtitle("Matriz 3")
```

```{r, echo = FALSE, warning=FALSE, message=FALSE, fig.width=5}
data.frame(
  Matriz = c("1", "2", "3"),
  Tasa = round(c(matriz1$tasa9, matriz2$tasa9, matriz3$tasa9), digits = 2)
) %>%
  kable(align = "c", booktabs = TRUE) %>%
  kable_styling(bootstrap_options = c("striped","hover","condensed")) %>%
  add_header_above(c(" " = 1, "Tabla 6: Tasa de aceptación según matriz de variancias y covariancias" = 1))
```

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.height=2.8}
grid.arrange(grafico9_1_1, grafico9_2_1, grafico9_3_1, ncol = 3, 
             bottom = "Gráfico 11: Autocorrelación de las muestras del parámetro a")
grid.arrange(grafico9_1_2, grafico9_2_2, grafico9_3_2, ncol = 3, 
             bottom = "Gráfico 12: Autocorrelación de las muestras del parámetro b")
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
#Punto 10
muestra9 <- as.data.frame(matriz1$muestra9)
colnames(muestra9) <- c("X", "Y")

#Probabilidades estimadas
#Probabilidad n°1:
prob1 <- muestra9 %>% 
  filter(X > 0 & X < 1 ) %>% 
  count(Y < 1 & Y > 0)

p1 <- prob1$n[2]/(n9)



#Probabilidad n°2:
prob2 <- muestra9 %>% 
  filter(X > -1 & X < 0 ) %>% 
  count(Y < 1 & Y > 0)

p2 <- prob2$n[2]/(n9)




#Probabilidad n°3:
prob3 <- muestra9 %>% 
  filter(X > 1 & X < 2 ) %>% 
  count(Y < 3 & Y > 2)

p3 <- prob3$n[2]/(n9)
p3 <- 0
```

Se vuelven a calcular las probabilidades anteriores, esta vez utilizando la integración de Monte Carlo:

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# Punto 11
# Calculo P(0<X1<1, 0<X2<1)

k <- 10000
u <- matrix(runif(k*2), ncol = 2)
p_estrella_eval <- numeric(k)

for (i in 1:k) {
  p_estrella_eval[i] <- p_estrella(u[i,], 0.5, 5)
}

rdo_prob1 <- mean(p_estrella_eval)

# Calculo P(-1<X1<0, 0<X2<1)

u <- matrix(c(runif(k, min = -1, max = 0),
              runif(k, min = 0, max = 1)),
            ncol = 2)

for (i in 1:k) {
  p_estrella_eval[i] <- p_estrella(u[i,], 0.5, 5)
}

rdo_prob2 <- mean(p_estrella_eval)

# Calculo P(1<X1<2, 2<X2<3)

u <- matrix(c(runif(k, min = 1, max = 2),
              runif(k, min = 2, max = 3)),
            ncol = 2)

for (i in 1:k) {
  p_estrella_eval[i] <- p_estrella(u[i,], 0.5, 5)
}

rdo_prob3 <- mean(p_estrella_eval)
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
data.frame (
  Probabilidad = c("i","ii","iii"),
  Muestra = round(c(p1,p2,p3), digits = 2),
  Monte_Carlo = round(c(rdo_prob1,rdo_prob2,rdo_prob3), digits = 2)
) %>%
  kable(align = "c", col.names = c("Probabilidad", "Muestra", "Monte-Carlo")) %>%
  kable_styling(bootstrap_options = c("striped","hover","condensed"),
                latex_options = c("hold_position")) %>%
  add_header_above(c(" " = 1, "Tabla 7: Probabilidades estimadas por la muestra y el método de Monte-Carlo" = 1))

```
